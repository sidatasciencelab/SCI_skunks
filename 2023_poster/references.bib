@article{abdulla2017mask,
  title={Mask R-CNN for object detection and instance segmentation on Keras and TensorFlow},
  author={Abdulla, Waleed},
  year={2017}
}

@software{Beery_Efficient_Pipeline_for,
author = {Beery, Sara and Morris, Dan and Yang, Siyu},
license = {MIT},
title = {{Efficient Pipeline for Camera Trap Image Review}},
url = {http://github.com/ecologize/CameraTraps}
}

@techreport{bubnicki_camtrap_2023,
	type = {preprint},
	title = {Camtrap {DP}: {An} open standard for the {FAIR} exchange and archiving of camera trap data},
	shorttitle = {Camtrap {DP}},
	url = {https://ecoevorxiv.org/repository/view/5593/},
	abstract = {Camera trapping has revolutionized wildlife ecology and conservation by providing automated data acquisition, leading to the accumulation of massive amounts of camera trap data worldwide. Although management and processing of camera trap-derived Big Data are becoming increasingly solvable with the help of scalable cyber-infrastructures, harmonization and exchange of the data remain limited, hindering its full potential. We present a new data exchange format, the Camera Trap Data Package (Camtrap DP), designed to allow users to easily exchange, harmonize and archive camera trap data at local to global scales. Camtrap DP structures camera trap data in a simple yet flexible data model consisting of three tables (Deployments, Media, and Observations) that supports a wide range of camera deployment designs, classification techniques (e.g., human and AI, media-based and event-based) and analytical use cases, from compiling species occurrence data through distribution, occupancy and activity modeling to density estimation. The format further achieves interoperability by building upon existing standards, Frictionless Data Package in particular, which is supported by a suite of open software tools to read and validate data. Camtrap DP is the consensus of a long, in-depth, consultation and outreach process with standard and software developers, the main existing camera trap data management platforms, major players in the field of camera trapping, and the Global Biodiversity Information Facility (GBIF). Under the umbrella of the Biodiversity Information Standards (TDWG), Camtrap DP has been developed openly, collaboratively, and with version control from the start and we encourage camera trapping users and developers to join the discussion and contribute to the further development and adoption of this standard.},
	institution = {EcoEvoRxiv},
	author = {Bubnicki, Jakub W. and Norton, Ben and Baskauf, Steven J. and Bruce, Tom and Cagnacci, Francesca and Casaer, Jim and Churski, Marcin and Cromsigt, Joris P.G.M. and Dal Farra, Simone and Fiderer, Christian and Forrester, Tavis D. and Hendry, Heidi and Heurich, Marco and Hofmeester, Tim R. and Jansen, Patrick A. and Kays, Roland and Kuijper, Dries P.J. and Liefting, Yorick and Linnell, John D.C. and Luskin, Matthew S. and Mann, Christopher and Milotic, Tanja and Newman, Peggy and Niedballa, JÃ¼rgen and Oldoni, Damiano and Ossi, Federico and Robertson, Tim and Rovero, Francesco and Rowcliffe, Marcus and Seidenari, Lorenzo and Stachowicz, Izabela and Stowell, Dan and Tobler, Mathias W. and Wieczorek, John and Zimmermann, Fridolin and Desmet, Peter},
	month = jun,
	year = {2023},
	doi = {10.32942/X2BC8J},
}

@inproceedings{girshick2015fast,
  title={Fast r-cnn},
  author={Girshick, Ross},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={1440--1448},
  year={2015}
}

@article{johanns2022automated,
  title={Automated distance estimation for wildlife camera trapping},
  author={Johanns, Peter and Haucke, Timm and Steinhage, Volker},
  journal={Ecological Informatics},
  volume={70},
  pages={101734},
  year={2022},
  publisher={Elsevier}
}

@article{keitt2021ecology,
  title={Ecology in the age of automation},
  author={Keitt, Timothy H and Abelson, Eric S},
  journal={Science},
  volume={373},
  number={6557},
  pages={858--859},
  year={2021},
  publisher={American Association for the Advancement of Science}
}

@article{velez2023evaluation,
  title={An evaluation of platforms for processing camera-trap data using artificial intelligence},
  author={V{\'e}lez, Juliana and McShea, William and Shamon, Hila and Castiblanco-Camacho, Paula J and Tabak, Michael A and Chalmers, Carl and Fergus, Paul and Fieberg, John},
  journal={Methods in Ecology and Evolution},
  volume={14},
  number={2},
  pages={459--477},
  year={2023},
  publisher={Wiley Online Library}
}

@Article{computers11010013,
AUTHOR = {Zualkernan, Imran and Dhou, Salam and Judas, Jacky and Sajun, Ali Reza and Gomez, Brylle Ryan and Hussain, Lana Alhaj},
TITLE = {An IoT System Using Deep Learning to Classify Camera Trap Images on the Edge},
JOURNAL = {Computers},
VOLUME = {11},
YEAR = {2022},
NUMBER = {1},
ARTICLE-NUMBER = {13},
URL = {https://www.mdpi.com/2073-431X/11/1/13},
ISSN = {2073-431X},
ABSTRACT = {Camera traps deployed in remote locations provide an effective method for ecologists to monitor and study wildlife in a non-invasive way. However, current camera traps suffer from two problems. First, the images are manually classified and counted, which is expensive. Second, due to manual coding, the results are often stale by the time they get to the ecologists. Using the Internet of Things (IoT) combined with deep learning represents a good solution for both these problems, as the images can be classified automatically, and the results immediately made available to ecologists. This paper proposes an IoT architecture that uses deep learning on edge devices to convey animal classification results to a mobile app using the LoRaWAN low-power, wide-area network. The primary goal of the proposed approach is to reduce the cost of the wildlife monitoring process for ecologists, and to provide real-time animal sightings data from the camera traps in the field. Camera trap image data consisting of 66,400 images were used to train the InceptionV3, MobileNetV2, ResNet18, EfficientNetB1, DenseNet121, and Xception neural network models. While performance of the trained models was statistically different (Kruskal&ndash;Wallis: Accuracy H(5) = 22.34, p &lt; 0.05; F1-score H(5) = 13.82, p = 0.0168), there was only a 3% difference in the F1-score between the worst (MobileNet V2) and the best model (Xception). Moreover, the models made similar errors (Adjusted Rand Index (ARI) &gt; 0.88 and Adjusted Mutual Information (AMU) &gt; 0.82). Subsequently, the best model, Xception (Accuracy = 96.1%; F1-score = 0.87; F1-Score = 0.97 with oversampling), was optimized and deployed on the Raspberry Pi, Google Coral, and Nvidia Jetson edge devices using both TenorFlow Lite and TensorRT frameworks. Optimizing the models to run on edge devices reduced the average macro F1-Score to 0.7, and adversely affected the minority classes, reducing their F1-score to as low as 0.18. Upon stress testing, by processing 1000 images consecutively, Jetson Nano, running a TensorRT model, outperformed others with a latency of 0.276 s/image (s.d. = 0.002) while consuming an average current of 1665.21 mA. Raspberry Pi consumed the least average current (838.99 mA) with a ten times worse latency of 2.83 s/image (s.d. = 0.036). Nano was the only reasonable option as an edge device because it could capture most animals whose maximum speeds were below 80 km/h, including goats, lions, ostriches, etc. While the proposed architecture is viable, unbalanced data remain a challenge and the results can potentially be improved by using object detection to reduce imbalances and by exploring semi-supervised learning.},
DOI = {10.3390/computers11010013}
}
